---
title: 'Capstone project in Natural Language Processing'
author: "Ariel Lev, 24. July 2015"
output: 
  html_document:
    keep_md: true
    css: style.css
    pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
    ]    
---

This document highlights a roadmap for developing a language model obtained from a corpus called [HC Corpora](www.corpora.heliohost.org)
```{r, echo=F, message=F, warning=F}
require(dplyr)
require(R.utils)
require(hash)
require(Matrix)
require(parallel) 
require(doParallel)
require(ggplot2)
require(knitr)
require(RWeka)
require(tm)
require(stringi)
```

##Data acquisition and cleaning
In order to reduce the running time involved with performing basic string manipulations, I tend to out-source this preliminary taks completely to unix' powerful built-in tools. For example, the following chunk demonstrates how *Awk* can be used to draw a 5% line sample out of an entire text file. The *tr* command accomplishes the task of removing most of the non-alphabetical characters. 
```{r, echo=F, message=F, warning=F, cache=T,nu}
print("awk 'rand() < 0.05 {print $0}' input > sample")
print("tr -sc '[a-zA-Z' .\n]' ' '")
```
Only when I had to apply more complex tasks such as filtering or tokenization that I switched to R to utilize libraries such as *Rweka* and *tm*. That will take us to the next topic.

###Framework
```{r, echo=T, message=F, warning=F, cache=T}

# downloading dataset
# download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", method="curl", destfile="swiftKey.zip")
# downloaded Di  7 Jul 2015 23:27:21 CEST

# the function samples lines from text file X by flipping a coin with a probability p for success
# the sample file is written to the disk.
createSamples <- function (X, p, ...) 
{
  awk <- paste0("awk 'rand() < ", p, " {print $0}' ", X, " > ", X, ".sample")
  system(awk)
  paste0(X, ".sample")
}

# basic string manipulations
cleanup <- function(X) {
    pipe <-readLines("cleanup", 1)
    
    output <- paste0(X, ".cleanup")  
    command <- paste("cat", X, pipe, ">", output)
    system(command)
    output
}

# script
readLines("cleanup", n = 1)
# explanation
# 1. sed 's/[^ ]*www[^ ]*/ /g'
# 2. tr "?\!" "."
# 3. tr -sc "[a-zA-Z' .\n]" " "
# 4. tr -s ". "
# 5. gsed -e 's/\.[ ]*\([A-Z]\)/.\L\1/g'
# 6. sed -e "s/ [a-zA-Z]\([^a-zA-Z]\)/\1/g"
# 7. sed -e "s/ [^.a-zA-Z][^ ]*/ /g"
# 8. tr '.' '\n'
# 9. awk "length($1) > 1 {print $1}"

# 1. removal of urls
# 2. Question and exclamation marks terminate sentences like dots do. we can therefore replace them with dots.
# 3. replace non alphabetic chars but keep apostrophes and dots
# 4. squeeze dots and spaces
# 5. using gnu-sed to transform upper to lower case - only for first letter in a sentence
# 6. removal of single char words
# 7. removal of words begin with an apostrophe
# 8. breaking lines on dots
# 9. removal of empty and single char lines

# the function filters a data vector by eliminating profanity and by checking existence against a dictionary.
filter <- function( data, profanity = c("*"), dictionary = c("*")) {
  cluster <- makeCluster(detectCores() - 1)
  registerDoParallel(cluster)
  
  filtered <- c()
  for (i in 1:length(data)) {
      line <- data[i]
      split <- unlist(strsplit(line, "[ ]+"))

      idx <- which(nchar(split) > 0)
      split <- split[idx]
      
      if (length(split) > 0) {
        idx <- which(has.key(split, dictionary_hash))
        split <- split[idx]
        
        idx <- which(!has.key(split, profanity_hash))
        split <- split[idx]               
      }
      
      split <- paste(split, collapse = " ")
      filtered[i] <- split
  }
  stopCluster(cluster)  
  filtered[nchar(filtered) > 0]
}

# The function returns a data frame containing n-grams with their corresponding counts and frequencies in corpus
tokenize <- function(corpus, min, max) {
  
  tokenizer <- function(x) {NGramTokenizer(x, Weka_control(min = min, max = max))}
  
  options(mc.cores=1)
  cluster <- makeCluster(detectCores() - 1)
  registerDoParallel(cluster)
  dtm  <- DocumentTermMatrix(corpus, control = list(tokenize = tokenizer, wordLengths = c( 1, Inf)))
  stopCluster(cluster)
  
  sM <- sparseMatrix(i=dtm$i, j = dtm$j, x = dtm$v)
  dt <- data.frame( term = as.character(dtm$dimnames$Terms), count = colSums(sM))
  dt$term <- as.character(dt$term)
  dt$n <- sapply( dt$term, function(x) {length(strsplit(x, "[ ]+")[[1]])})
  
  dt <- dt[order(-dt$count),]
  dt
}

plot_ngrams <- function(ngram, nrows, title, fill) {
  ngram$term <- factor(ngram$term, levels = ngruam$term)
  ggplot(head(ngram, nrows), aes(x = term, y = freq, fill=doc)) + geom_bar(stat = "Identity", fill = fill) +  ggtitle(title) + theme(axis.text.x = element_text(angle = 90, hjust = 1), axis.title.x=element_blank(),axis.title.y=element_blank())  
}

plot_combined <- function(ngrams, title) {
  ordr <- group_by(ngrams, term) %>% summarize(sum = sum(count)) %>% arrange(desc(sum))
  ngrams$term <- factor(ngrams$term, levels = ordr$term)
  ggplot(ngrams, aes(x = term, y = count, fill = doc)) + geom_bar(stat = "Identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1), axis.title.x=element_blank(),axis.title.y=element_blank()) + ggtitle(title)
}

suffix <- function(mStr, n) {
  if (n == 0) {
    return (".*")
  }
  vector <- unlist(strsplit(mStr, "[ ]+"))  
  suff <- stri_flatten(vector[(length(vector)-n+1):length(vector)], collapse = " ")
  suff
}


prefix_str <- function(mStr, n) {
  sapply(mStr, function(x){ v <- unlist(strsplit(x, "[ ]+")); m <- min(length(v), n); stri_flatten(v[1:m], collapse = " ")})
}

probability <- function(master, mStr, n) {  
    suff <- suffix(mStr, n - 1)  
    idx <- grepl(paste0("^(",suff, " )"), master$term) & master$n == n
    master[idx, ]
}

# uses stupid backoff
backoff <- function(master, prefixes, threshold = 0) {
  m <- master[master$p > threshold, ]
  predictions <- list()
  for (i in 1:length(prefixes)) {
  df <- data.frame(term = character(), count = numeric(), n = numeric(), prefix = character(), p = numeric(), next_term = character())    
    prefix <- prefixes[i]
    vector <- unlist(strsplit(prefix, "[ ]+")) 
    for (k in 3:1) {
      a <- probability(m, suffix(prefix,k), k+1)
      a$p <- a$p*0.4^(3-k)
      a <- a[a$p > threshold, ]
      a$next_term <- sapply(a$term, function(x) { v <- unlist(strsplit(x, "[ ]+")); v[length(v)]} )
      df <- rbind(df, a)
    }
    predictions[[i]] <- df
  }
  predictions
}

next_word <- function(master, prefixes, threshold) {
  l <- backoff(master, prefixes, threshold)
  lapply(l, function(x) { unique(x$next_term)})
}

# calculates likelihood using relative frequencies
MLE <- function(df, df_prefix) {
  n <- df[1,3]$n
  total <- sum(df$count)
  
  h_counts <- hash(df_prefix$term, df_prefix$count)
  df$prefix <- prefix_str(df$term, n - 1)
  
  # setting deafult value
  df$p <- total
  df$p <- sapply(df$prefix, function(x) {d <- h_counts[[x]]; if (!is.null(d)) return(d) else return (-1)})
  df %>% mutate(p = ifelse(p == -1, -1, count / p))
}

smooth_add_one_p <- function(df, df_prefix) {  
  n <- df[1,3] 
  v <- nrow(df_prefix)  
  h_counts <- hash(df_prefix$term, df_prefix$count)
  df$prefix <- prefix_str(df$term, n - 1)
  df$p <- v
  df$p <- sapply(df$prefix, function(x) {d <- h_counts[[x]]; if (!is.null(d)) return(d) else return (-1)})
  df %>% mutate(p = ifelse(p == -1, -1, (count + 1) / (p + v) ))  
}

smooth_add_one_count <- function(df) {  
  N <- sum(df$count)
  df$count_add_1 <- (N * (df$count + 1)) / (N+nrow(df))
  df
}

smooth_witten_bell <- function(df) {
  df
}

smooth_good_turing <- function(df) {
  df
}
```

### Data Processing
The following unix command outputs line and word count per data source.  
"Blogs" contains for example almost 900k lines and over 3.7M words. Total number of lines is approximately 4.26M and word count is around 102M
```{r, echo=T, fig.width=12, fig.height=4, message=F, warning=F}
console <- system("find . -name '*en_US*txt' | xargs wc -wl", intern = T)
for (i in 1:length(console)) print(console[i])
```

```{r, echo=T, message=F, warning=F, cache=T}
# creating samples of 2% of the total lines
samples <- Sys.glob("final/en_US/en_US*.sample")
if (length(samples) < 3) {
  input <- Sys.glob("final/en_US/en_US*.txt")
  samples <- unlist(lapply(input, function(x) createSamples(x,.05)))
}
# 
# cleaning up
cleaned <- Sys.glob("final/en_US/*.cleanup")
if (length(cleaned) < 3) {
  cleaned <- unlist(lapply(samples, function(x) cleanup(x)))
}

# filter
profanity <- tolower(readLines("final/en_US/profanity.txt"))
head(profanity, 30)
profanity_hash <- hash(keys = profanity, values=rep("", length(profanity)))

dictionary <- tolower(readLines("final/en_US/dictionary.txt"))
head(dictionary, 30)
dictionary_hash <- hash(keys = dictionary, values=rep("", length(dictionary))) 

# processing documents
all = list()
documents <- c("twitter", "blogs", "news")
colors <- c("deepskyblue", "darkgrey", "gold3")

for (i in 1:length(documents)) {
  d <- documents[i]
  raw <- tolower(readLines(sprintf('final/en_US/en_US.%s.txt.sample.cleanup', d)))
  filtered <- filter(raw, profanity_hash, dictionary_hash)
  corpus <- Corpus(VectorSource(filtered))
  
  # tidying up a bit more
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, PlainTextDocument)
  
  tokenized <- tokenize(corpus, 1, 4)
  
  unigrams <- tokenized[tokenized$n == 1, ]
  bigrams <- tokenized[tokenized$n == 2, ]
  trigrams <- tokenized[tokenized$n == 3, ]
  quadrigrams <- tokenized[tokenized$n == 4, ]
  
  unigrams$doc <- d
  unigrams$colors <- colors[i]  
  bigrams$doc <- d
  bigrams$colors <- colors[i]
  trigrams$doc <- d
  trigrams$colors <- colors[i]
  quadrigrams$doc <- d
  quadrigrams$colors <- colors[i]

  all[[d]]$corpus <- corpus
  all[[d]]$unigrams <- unigrams
  all[[d]]$bigrams <- bigrams
  all[[d]]$trigrams <- trigrams
  all[[d]]$quadrigrams <- quadrigrams  
  all[[d]]$color <- colors[i]

}
```

##Results
###Total n-grams per document type
```{r, echo=F, fig.width=12, fig.height=4, message=F, warning=F}
# total n-grams per document type
sapply(all, function(x) list( uni = nrow(x$unigrams), bi = nrow(x$bigrams), tri = nrow(x$trigrams)))
```

Frequencies of terms plotted below reveal more distinctive patterns as n increases.
```{r, echo=F, fig.width=12, fig.height=8, message=F, warning=F}
unigrams <- rbind(all$twitter$unigrams[1:40,],all$blogs$unigrams[1:40,],all$news$unigrams[1:40,])
plot_combined(unigrams, "Frequent unigrams")

bigrams <- rbind(all$twitter$bigrams[1:40,],all$blogs$bigrams[1:40,],all$news$bigrams[1:40,])
plot_combined(bigrams, "Frequent bigrams")

trigrams <- rbind(all$twitter$trigrams[1:40,],all$blogs$trigrams[1:40,],all$news$trigrams[1:40,])
plot_combined(trigrams, "Frequent trigrams")
```


```{r, echo=F, fig.width=12, fig.height=4, message=F, warning=F}

unigrams <- rbind(all$twitter$unigrams, all$news$unigrams, all$blogs$unigrams)
bigrams <- rbind(all$twitter$bigrams, all$news$bigrams, all$blogs$bigrams)
trigrams <- rbind(all$twitter$trigrams, all$news$trigrams, all$blogs$trigrams)
quadrigrams <- rbind(all$twitter$quadrigrams, all$news$quadrigrams, all$blogs$quadrigrams)

gu <- group_by(unigrams, term) %>% summarize(count = sum(count), n = 1) %>% arrange(desc(count))
gb <- group_by(bigrams, term) %>% summarize(count = sum(count), n = 2) %>% arrange(desc(count))
gt <- group_by(trigrams, term) %>% summarize(count = sum(count), n = 3) %>% arrange(desc(count))
gq <- group_by(quadrigrams, term) %>% summarize(count = sum(count), n = 4) %>% arrange(desc(count))

gu$prefix <- ""
gu$p <- gu$count / sum(gu$count)

gb <- relative_frequencies(df = gb, df_prefix = gu)
gt <- relative_frequencies(df = gt, df_prefix = gb)
gq <- relative_frequencies(df = gq, df_prefix = gt)


write.table(gu, file="gu", row.names = F)
write.table(gb, file="gb", row.names = F)
write.table(gt, file="gt", row.names = F)
write.table(gq, file="gq", row.names = F)

master <- rbind(gu, gb, gt, gq)
write.table(master, file="master", row.names = F)

rm(gu)
rm(gb)
rm(gt)
rm(gq)

```

```{r, echo=T, message=F, warning=F, cache=T}
# quiz3

evaluate_quiz <- function(master, quiz, threshold = 0.01) {
  l <- next_word(master, quiz[,1], 0.001)
  vector <- rep(NA, nrow(quiz))
  for (i in 1:nrow(quiz3)) {
    vector[i] <- paste(intersect(l[[i]], c(quiz[i,2], quiz[i,3], quiz[i,4], quiz[i,5])), collapse = " ")
  }
  vector
}
quiz3 <- read.csv("quiz3", stringsAsFactors = F, header = T, sep=";")
evaluate_quiz(master, quiz3, threshold = 0.01)
```
##What's next?
+ Scaling up the sample size from 2% in stages to 5%, 10%, 20% etc..
+ Applying Kneser-Ney smoothing to compute probability
+ Applying Linear interpolation
+ Supporting misspelling: Non-Word Errors, Real-Word Errors
+ Data compression and Data retrieval - using hashs, double hashs etc..